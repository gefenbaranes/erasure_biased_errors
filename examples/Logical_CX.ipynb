{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layers = 2\n",
      "num_CX_per_layer = 10\n",
      "generating the Logical circuit took: 0.462691s\n",
      "Logical circuit that will be used: \n",
      "R 1 2 3 7 8 9 13 14 15\n",
      "X_ERROR(5.8941e-05) 1 2 3 7 8 9 13 14 15\n",
      "I 1 2 3 7 8 9 13 14 15\n",
      "SQRT_Y_DAG 1 3 8 13 15\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05) 1 3 8 13 15\n",
      "R 18 19 20 24 25 26 30 31 32\n",
      "X_ERROR(5.8941e-05) 32 18 19 20 24 25 26 30 31\n",
      "I 32 18 19 20 24 25 26 30 31\n",
      "SQRT_Y_DAG 19 24 26 31\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05) 19 24 26 31\n",
      "SQRT_Y 2 7 9 14\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05) 2 7 9 14\n",
      "SQRT_Y 18 20 25 30 32\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05) 18 20 25 30 32\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492)\n",
      "I 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "CZ 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857)\n",
      "I 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 32\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492)\n",
      "I 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "CZ 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857)\n",
      "I 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 32\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492)\n",
      "I 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "CZ 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857)\n",
      "I 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 32\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492)\n",
      "I 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "CZ 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857)\n",
      "I 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 32\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492)\n",
      "I 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "CZ 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857)\n",
      "I 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 32\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492)\n",
      "I 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "CZ 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857)\n",
      "I 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 32\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492)\n",
      "I 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "CZ 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857)\n",
      "I 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 32\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492)\n",
      "I 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "CZ 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857)\n",
      "I 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 32\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492)\n",
      "I 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "CZ 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857)\n",
      "I 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 32\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492)\n",
      "I 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "CZ 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857)\n",
      "I 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 32\n",
      "SQRT_Y 2 7 9 14\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05) 2 7 9 14\n",
      "SQRT_Y 18 20 25 30 32\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05) 18 20 25 30 32\n",
      "R 4 6 10 12 21 23 27 29 0 5 11 16 17 22 28 33\n",
      "X_ERROR(0.0245492) 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "I 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "SQRT_Y 4 6 10 12 21 23 27 29 0 5 11 16 17 22 28 33\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05) 4 6 10 12 21 23 27 29 0 5 11 16 17 22 28 33\n",
      "CZ 1 0 8 5 13 11 4 7 6 9 12 14\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 1 0 8 5 13 11 4 7 6 9 12 14\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857) 2 3 10 15 16\n",
      "I 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\n",
      "SQRT_Y 1 2 3 7 8 9 13 14 15\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05) 1 2 3 7 8 9 13 14 15\n",
      "Y 23\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05)\n",
      "CZ 2 5 7 11 14 16 4 1 6 3 12 8\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 2 5 7 11 14 16 4 1 6 3 12 8\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857) 0 9 10 13 15\n",
      "I 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 1 2 3 7 8 9 13 14 15\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492) 0 4 5 6 10 11 12 16\n",
      "I 1 2 3 7 8 9 13 14 15 0 4 5 6 10 11 12 16\n",
      "Y 1 2 3 7 8 9 13 14 15 4 6 10 12 0 5 11 16\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05) 1 2 3 7 8 9 13 14 15 4 6 10 12 0 5 11 16\n",
      "CZ 2 0 9 5 14 11 4 8 10 13 12 15\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 2 0 9 5 14 11 4 8 10 13 12 15\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857) 1 3 6 7 16\n",
      "I 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 1 2 3 7 8 9 13 14 15\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492) 0 4 5 6 10 11 12 16\n",
      "I 1 2 3 7 8 9 13 14 15 0 4 5 6 10 11 12 16\n",
      "SQRT_Y 1 2 3 7 8 9 13 14 15\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05) 1 2 3 7 8 9 13 14 15\n",
      "Y 27\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05)\n",
      "CZ 3 5 8 11 15 16 4 2 10 7 12 9\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 3 5 8 11 15 16 4 2 10 7 12 9\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857) 0 1 6 13 14\n",
      "I 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 1 2 3 7 8 9 13 14 15\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492) 0 4 5 6 10 11 12 16\n",
      "I 1 2 3 7 8 9 13 14 15 0 4 5 6 10 11 12 16\n",
      "CZ 18 17 25 22 30 28 21 24 23 26 29 31\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 18 17 25 22 30 28 21 24 23 26 29 31\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857) 32 33 19 20 27\n",
      "I 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33\n",
      "SQRT_Y 18 19 20 24 25 26 30 31 32\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05) 18 19 20 24 25 26 30 31 32\n",
      "Y 23\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05) 23\n",
      "CZ 19 22 24 28 31 33 21 18 23 20 29 25\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 19 22 24 28 31 33 21 18 23 20 29 25\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857) 32 17 26 27 30\n",
      "I 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492) 33 17 21 22 23 27 28 29\n",
      "I 32 18 19 20 24 25 26 30 31 33 17 21 22 23 27 28 29\n",
      "Y 18 19 20 24 25 26 30 31 32 21 27 29 17 22 28 33\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05) 18 19 20 24 25 26 30 31 32 21 27 29 17 22 28 33\n",
      "CZ 19 17 26 22 31 28 21 25 27 30 29 32\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 19 17 26 22 31 28 21 25 27 30 29 32\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857) 33 18 20 23 24\n",
      "I 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492) 33 17 21 22 23 27 28 29\n",
      "I 32 18 19 20 24 25 26 30 31 33 17 21 22 23 27 28 29\n",
      "SQRT_Y 18 19 20 24 25 26 30 31 32\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05) 18 19 20 24 25 26 30 31 32\n",
      "Y 27\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05) 27\n",
      "CZ 20 22 25 28 32 33 21 19 27 24 29 26\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 20 22 25 28 32 33 21 19 27 24 29 26\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857) 17 18 23 30 31\n",
      "I 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492) 33 17 21 22 23 27 28 29\n",
      "I 32 18 19 20 24 25 26 30 31 33 17 21 22 23 27 28 29\n",
      "SQRT_Y 4 6 10 12 21 23 27 29 0 5 11 16 17 22 28 33\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05) 4 6 10 12 21 23 27 29 0 5 11 16 17 22 28 33\n",
      "Y 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 32 4 6 10 12 21 27 29 0 5 11 16 17 22 28 33\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05) 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 32 4 6 10 12 21 27 29 0 5 11 16 17 22 28 33\n",
      "X_ERROR(0.00128159) 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "I 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "M 0 4 5 6 10 11 12 16 17 21 22 23 27 28 29 33\n",
      "DETECTOR rec[-15] rec[-7]\n",
      "DETECTOR rec[-13] rec[-5]\n",
      "DETECTOR rec[-12] rec[-4]\n",
      "DETECTOR rec[-10] rec[-2]\n",
      "DETECTOR rec[-16] rec[-8]\n",
      "DETECTOR rec[-14] rec[-6]\n",
      "DETECTOR rec[-11] rec[-3]\n",
      "DETECTOR rec[-9] rec[-1]\n",
      "SQRT_Y 2 7 9 14\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05) 2 7 9 14\n",
      "SQRT_Y 18 20 25 30 32\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05) 18 20 25 30 32\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492) 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "I 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "CZ 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857) 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "I 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492) 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "I 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "CZ 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857) 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "I 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492) 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "I 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "CZ 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857) 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "I 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492) 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "I 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "CZ 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857) 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "I 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492) 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "I 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "CZ 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857) 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "I 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492) 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "I 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "CZ 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857) 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "I 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492) 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "I 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "CZ 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857) 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "I 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492) 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "I 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "CZ 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857) 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "I 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492) 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "I 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "CZ 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857) 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "I 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33\n",
      "PAULI_CHANNEL_1(1.3211e-06, 6.7667e-06, 5.35053e-05) 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "PAULI_CHANNEL_1(2.93451e-05, 9.21782e-06, 0.000460492) 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "I 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "CZ 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_2(2.22607e-05, 0.000171396, 0.00129483, 2.22607e-05, 0, 0, 0, 0.000171396, 0, 0, 0, 0.00129483, 0, 0, 0.00262174) 1 18 2 19 3 20 7 24 8 25 9 26 13 30 14 31 15 32\n",
      "PAULI_CHANNEL_1(0.000366476, 6.14733e-06, 0.00235857) 0 33 4 5 6 10 11 12 16 17 21 22 23 27 28 29\n",
      "I 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33\n",
      "SQRT_Y 2 7 9 14\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05) 2 7 9 14\n",
      "SQRT_Y 18 20 25 30 32\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05) 18 20 25 30 32\n",
      "SQRT_Y_DAG 1 3 8 13 15 18 20 25 30 32\n",
      "PAULI_CHANNEL_1(1.53681e-05, 0.000993583, 1.9465e-05) 1 3 8 13 15 18 20 25 30 32\n",
      "X_ERROR(0.00061387) 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "I 32 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31\n",
      "M 1 2 3 7 8 9 13 14 15 18 19 20 24 25 26 30 31 32\n",
      "DETECTOR rec[-18] rec[-17] rec[-15] rec[-14] rec[-33] rec[-25]\n",
      "DETECTOR rec[-16] rec[-13] rec[-31] rec[-23]\n",
      "DETECTOR rec[-15] rec[-12] rec[-30] rec[-22]\n",
      "DETECTOR rec[-14] rec[-13] rec[-11] rec[-10] rec[-28] rec[-20]\n",
      "DETECTOR rec[-9] rec[-8] rec[-6] rec[-5] rec[-25]\n",
      "DETECTOR rec[-7] rec[-4] rec[-23]\n",
      "DETECTOR rec[-6] rec[-3] rec[-22]\n",
      "DETECTOR rec[-5] rec[-4] rec[-2] rec[-1] rec[-20]\n",
      "OBSERVABLE_INCLUDE(0) rec[-15] rec[-14] rec[-13] rec[-6] rec[-5] rec[-4]\n",
      "len potential_lost_qubits: 1346\n",
      "potential_lost_qubits: [1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 32.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 32.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 32.0, 32.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 32.0, 32.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 32.0, 32.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 32.0, 32.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 32.0, 32.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 32.0, 32.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 32.0, 32.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 32.0, 32.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 32.0, 32.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 32.0, 0.0, 33.0, 4.0, 5.0, 6.0, 10.0, 11.0, 12.0, 16.0, 17.0, 21.0, 22.0, 23.0, 27.0, 28.0, 29.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 0.0, 4.0, 5.0, 6.0, 10.0, 11.0, 12.0, 16.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 0.0, 4.0, 5.0, 6.0, 10.0, 11.0, 12.0, 16.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 0.0, 4.0, 5.0, 6.0, 10.0, 11.0, 12.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 32.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 33.0, 17.0, 21.0, 22.0, 23.0, 27.0, 28.0, 29.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 32.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 33.0, 17.0, 21.0, 22.0, 23.0, 27.0, 28.0, 29.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 32.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 33.0, 17.0, 21.0, 22.0, 23.0, 27.0, 28.0, 29.0, 0.0, 33.0, 4.0, 5.0, 6.0, 10.0, 11.0, 12.0, 16.0, 17.0, 21.0, 22.0, 23.0, 27.0, 28.0, 29.0, 32.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 0.0, 33.0, 4.0, 5.0, 6.0, 10.0, 11.0, 12.0, 16.0, 17.0, 21.0, 22.0, 23.0, 27.0, 28.0, 29.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 32.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 0.0, 33.0, 4.0, 5.0, 6.0, 10.0, 11.0, 12.0, 16.0, 17.0, 21.0, 22.0, 23.0, 27.0, 28.0, 29.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 32.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 0.0, 33.0, 4.0, 5.0, 6.0, 10.0, 11.0, 12.0, 16.0, 17.0, 21.0, 22.0, 23.0, 27.0, 28.0, 29.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 32.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 0.0, 33.0, 4.0, 5.0, 6.0, 10.0, 11.0, 12.0, 16.0, 17.0, 21.0, 22.0, 23.0, 27.0, 28.0, 29.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 32.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 0.0, 33.0, 4.0, 5.0, 6.0, 10.0, 11.0, 12.0, 16.0, 17.0, 21.0, 22.0, 23.0, 27.0, 28.0, 29.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 32.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 0.0, 33.0, 4.0, 5.0, 6.0, 10.0, 11.0, 12.0, 16.0, 17.0, 21.0, 22.0, 23.0, 27.0, 28.0, 29.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 32.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 0.0, 33.0, 4.0, 5.0, 6.0, 10.0, 11.0, 12.0, 16.0, 17.0, 21.0, 22.0, 23.0, 27.0, 28.0, 29.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 32.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 0.0, 33.0, 4.0, 5.0, 6.0, 10.0, 11.0, 12.0, 16.0, 17.0, 21.0, 22.0, 23.0, 27.0, 28.0, 29.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 32.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 0.0, 33.0, 4.0, 5.0, 6.0, 10.0, 11.0, 12.0, 16.0, 17.0, 21.0, 22.0, 23.0, 27.0, 28.0, 29.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 32.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0, 0.0, 33.0, 4.0, 5.0, 6.0, 10.0, 11.0, 12.0, 16.0, 17.0, 21.0, 22.0, 23.0, 27.0, 28.0, 29.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 32.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0, 13.0, 14.0, 15.0, 18.0, 19.0, 20.0, 24.0, 25.0, 26.0, 30.0, 31.0]\n",
      "loss_probabilities: [0.00075311 0.00075311 0.00075311 ... 0.07131074 0.07131074 0.07131074]\n",
      "final measurement_index = 34\n",
      "Preprocessing is done! it took 23.69s\n",
      "Decoder initialized, it took 23.74s for everything\n",
      "Shot: 0 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kh/9b4nnp7x0s7c1fp6wn1qhrnm0000gn/T/ipykernel_20191/384985113.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/gefenbaranes/Documents/CX_experiment'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# DO IT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m predictions, observable_flips, dems_list = Loss_MLE_Decoder_Experiment(Meta_params, distance, distance, output_dir,\n\u001b[0m\u001b[1;32m     63\u001b[0m                                                                     \u001b[0mexp_measurements\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                                                                     \u001b[0mdetection_events_signs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_loss_decoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local_erasure_biased_errors/BiasedErasure/delayed_erasure_decoders/Experimental_Loss_Decoder.py\u001b[0m in \u001b[0;36mLoss_MLE_Decoder_Experiment\u001b[0;34m(Meta_params, dx, dy, output_dir, measurement_events, detection_events_signs, use_loss_decoding, use_independent_decoder, use_independent_and_first_comb_decoder, simulate_data, first_comb_weight, noise_params, logical_gaps, num_shots)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlogical_gaps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# Step 1 - decode:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 predictions, observable_flips, dems_list = simulator.count_logical_errors_experiment(num_shots = num_shots, dx = dx, dy = dy,\n\u001b[0m\u001b[1;32m     44\u001b[0m                                                         \u001b[0mmeasurement_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeasurement_events\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_events_signs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdetection_events_signs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                                                         \u001b[0muse_loss_decoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_loss_decoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local_erasure_biased_errors/BiasedErasure/main_code/Simulator.py\u001b[0m in \u001b[0;36mcount_logical_errors_experiment\u001b[0;34m(self, num_shots, dx, dy, measurement_events, detection_events_signs, use_loss_decoding, use_independent_decoder, use_independent_and_first_comb_decoder, simulate_data, noise_params)\u001b[0m\n\u001b[1;32m    603\u001b[0m                     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                     \u001b[0mreturn_matrix_with_observables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'MLE'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m                     \u001b[0mfinal_dem_hyperedges_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservables_errors_interactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLE_Loss_Decoder_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_dem_loss_mle_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeasurement_event\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_matrix_with_observables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_matrix_with_observables\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# final_dem_hyperedges_matrix doesn't contain observables, only detectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m                     \u001b[0;31m# print(f'Total loss decoder time per shot is {time.time() - start_time:.4f}s.')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local_erasure_biased_errors/BiasedErasure/delayed_erasure_decoders/MLE_Loss_Decoder.py\u001b[0m in \u001b[0;36mgenerate_dem_loss_mle_experiment\u001b[0;34m(self, measurement_event, return_matrix_with_observables)\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m11\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_type\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m  \u001b[0;34m'independent'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Independent decoder:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0mstart_time_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                 \u001b[0mfinal_dem_hyperedges_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_all_DEMs_and_sum_over_independent_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_hyperedges_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m                 \u001b[0;31m# print(f'Summing over all relevant DEMs to generate the final DEM took {time.time() - start_time_ind:.5f}s')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local_erasure_biased_errors/BiasedErasure/delayed_erasure_decoders/MLE_Loss_Decoder.py\u001b[0m in \u001b[0;36mgenerate_all_DEMs_and_sum_over_independent_events\u001b[0;34m(self, use_pre_processed_data, return_hyperedges_matrix, remove_gates_due_to_loss)\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;31m# DEBUG - try the high order equation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m             \u001b[0mDEM_specific_loss_event\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_DEMs_high_order_csr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEMs_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEMs_specific_loss_event\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_detectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_detectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProbs_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mProbs_specific_loss_event\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local_erasure_biased_errors/BiasedErasure/delayed_erasure_decoders/MLE_Loss_Decoder.py\u001b[0m in \u001b[0;36mcombine_DEMs_high_order_csr\u001b[0;34m(self, DEMs_list, num_detectors, Probs_list)\u001b[0m\n\u001b[1;32m   1552\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv3\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m                             \u001b[0mprob_i_terms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1555\u001b[0m             \u001b[0mfinal_rows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_i_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m         \u001b[0;31m# print(f'high order formula took {time.time() - start_time:.6f}s')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local_erasure_biased_errors/BiasedErasure/delayed_erasure_decoders/MLE_Loss_Decoder.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1552\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv3\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m                             \u001b[0mprob_i_terms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1555\u001b[0m             \u001b[0mfinal_rows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_i_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m         \u001b[0;31m# print(f'high order formula took {time.time() - start_time:.6f}s')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from BiasedErasure.delayed_erasure_decoders.Experimental_Loss_Decoder import *\n",
    "import numpy as np\n",
    "# num_rounds = 5\n",
    "num_layers = 2\n",
    "num_cxs_per_round = 10\n",
    "distance = 3\n",
    "decoder_basis = 'XX'\n",
    "gate_ordering = ['N', 'Z']\n",
    "noise_params = {'idle_loss_rate': 2.793300220405646e-07, 'idle_error_rate': np.array([6.60547942e-09, 3.38336163e-08, 2.67533789e-07]),\n",
    "                'entangling_zone_error_rate': np.array([3.66476387e-04, 6.14732819e-06, 2.35857048e-03]),\n",
    "                'entangling_gate_error_rate': [2.2260729018707513e-05, 0.00017139584089578063, 0.0012948317242757047, 2.2260729018707513e-05, 0, 0, 0, 0.00017139584089578063, 0, 0, 0, 0.0012948317242757047, 0, 0, 0.002621736717313752],\n",
    "                'entangling_gate_loss_rate': 0.00039272255674060926, 'single_qubit_error_rate': np.array([1.53681034e-05, 9.93583065e-04, 1.94650113e-05]),\n",
    "                'reset_error_rate': 5.89409983290463e-05, 'measurement_error_rate': 0.0006138700821647161, 'reset_loss_rate': 0.0007531131027610011, 'measurement_loss_rate': 0.07131074481520218, 'ancilla_idle_loss_rate': 1.6989311035347498e-07,\n",
    "                'ancilla_idle_error_rate': np.array([1.46727589e-07, 4.60893305e-08, 2.30298714e-06]), 'ancilla_reset_error_rate': 0.024549181355318986, 'ancilla_measurement_error_rate': 0.0012815874700447462, 'ancilla_reset_loss_rate': 0.00019528486460263086, 'ancilla_measurement_loss_rate': 0.00047357577582906143,\n",
    "                'gate_noise': LogicalCircuit.ancilla_data_differentiated_gate_noise, 'idle_noise': LogicalCircuit.ancilla_data_differentiated_idle_noise}\n",
    "\n",
    "\n",
    "\n",
    "Meta_params = {'architecture': 'CBQC', 'code': 'Rotated_Surface', 'logical_basis': decoder_basis,\n",
    "            'bias_preserving_gates': 'False',\n",
    "            'noise': 'atom_array', 'is_erasure_biased': 'False', 'LD_freq': '1000', 'LD_method': 'None',\n",
    "            'SSR': 'True', 'cycles': str(num_layers - 1),\n",
    "            'ordering': gate_ordering,\n",
    "            'decoder': 'MLE',\n",
    "            'circuit_type': f'logical_CX_NL{num_layers}_NCX{num_cxs_per_round}', 'Steane_type': 'None', 'printing': 'True', 'num_logicals': '2',\n",
    "            'loss_decoder': 'independent',\n",
    "            'obs_pos': 'd-1', 'n_r': '0'}\n",
    "\n",
    "\n",
    "\n",
    "# Load the experimental measurements\n",
    "\n",
    "simulate_data = True\n",
    "\n",
    "if simulate_data:\n",
    "    detection_events_signs = None\n",
    "    exp_measurements = None\n",
    "    num_shots = 1500\n",
    "\n",
    "else:\n",
    "    # Load the theory circuit\n",
    "    _, _, _, circuit = get_simulated_measurement_events(Meta_params, distance, distance, 1, noise_params)\n",
    "    # Use the theory circuit to get the detection events and observable flips corresponding to the exp data\n",
    "    detection_events, observable_flips = circuit.compile_m2d_converter().convert(measurements=exp_measurements.astype(bool), separate_observables=True)\n",
    "    # Find detection event signs\n",
    "    detection_events_signs = -np.sign(2*np.nanmean(detection_events.astype(int), axis=0)-1).astype(int)\n",
    "    exp_measurements = np.load(f'/Users/gefenbaranes/Documents/2024_10_15_measurement_events_1CNOT_XX.npy')\n",
    "    exp_measurements = np.concatenate([exp_measurements[:, 0, :distance**2-1],\n",
    "                                    exp_measurements[:, 1, :distance**2-1],\n",
    "                                    exp_measurements[:, 0, distance**2-1:2*(distance**2-1)],\n",
    "                                    exp_measurements[:, 1, distance**2-1:2*(distance**2-1)],\n",
    "                                    exp_measurements[:, 0, 2*(distance**2-1):],\n",
    "                                    exp_measurements[:, 1, 2*(distance**2-1):]], axis=1)\n",
    "\n",
    "\n",
    "# Now let's decode!\n",
    "use_loss_decoding = True  # if False: use same DEM every shot, without utilizing SSR.\n",
    "use_independent_decoder = True  # if False: in every lifecycle, we just apply supercheck at the end. If True: we count the full lifecycle with different potential loss locations and corresponding Clifford propagations.\n",
    "use_independent_and_first_comb_decoder = False  # This is relevant only if use_independent_decoder=True. If False: use only independent lifecycles. If True: adds a single combination of lifecycles to the decoder.\n",
    "output_dir = '/Users/gefenbaranes/Documents/CX_experiment'\n",
    "# DO IT\n",
    "predictions, observable_flips, dems_list = Loss_MLE_Decoder_Experiment(Meta_params, distance, distance, output_dir,\n",
    "                                                                    exp_measurements,\n",
    "                                                                    detection_events_signs, use_loss_decoding,\n",
    "                                                                    use_independent_decoder,\n",
    "                                                                    use_independent_and_first_comb_decoder,\n",
    "                                                                    simulate_data=simulate_data, logical_gaps=False,\n",
    "                                                                    noise_params=noise_params, num_shots=num_shots)\n",
    "logical_probability = np.mean(np.logical_xor(observable_flips, predictions))\n",
    "print('/n logical error', logical_probability)\n",
    "\n",
    "\n",
    "# error bar: (np.sqrt(P*(1-P)/num_shots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "num_layers = 20\n",
    "num_cxs_per_round = 0\n",
    "circuit_type = f'logical_CX_NL{num_layers}_NCX{num_cxs_per_round}'\n",
    "num_layers, num_cxs_per_round = map(int, re.findall(r'\\d+', circuit_type))\n",
    "print(num_layers)\n",
    "print(num_cxs_per_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9553333333333334"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-logical_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layers = 3\n",
      "num_CX_per_layer = 3\n",
      "final measurement_index = 146\n",
      "0 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kh/9b4nnp7x0s7c1fp6wn1qhrnm0000gn/T/ipykernel_16894/3841985880.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# DO IT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m predictions, observable_flips, dems_list = Loss_MLE_Decoder_Experiment(Meta_params, distance, distance, output_dir,\n\u001b[0m\u001b[1;32m     66\u001b[0m                                                                     \u001b[0mexp_measurements\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                                                                     \u001b[0mdetection_events_signs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_loss_decoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local_erasure_biased_errors/BiasedErasure/delayed_erasure_decoders/Experimental_Loss_Decoder.py\u001b[0m in \u001b[0;36mLoss_MLE_Decoder_Experiment\u001b[0;34m(Meta_params, dx, dy, output_dir, measurement_events, detection_events_signs, use_loss_decoding, use_independent_decoder, use_independent_and_first_comb_decoder, simulate_data, first_comb_weight, noise_params, logical_gaps, num_shots)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlogical_gaps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# Step 1 - decode:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 predictions, observable_flips, dems_list = simulator.count_logical_errors_experiment(num_shots = num_shots, dx = dx, dy = dy,\n\u001b[0m\u001b[1;32m     44\u001b[0m                                                         \u001b[0mmeasurement_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeasurement_events\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_events_signs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdetection_events_signs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                                                         \u001b[0muse_loss_decoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_loss_decoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local_erasure_biased_errors/BiasedErasure/main_code/Simulator.py\u001b[0m in \u001b[0;36mcount_logical_errors_experiment\u001b[0;34m(self, num_shots, dx, dy, measurement_events, detection_events_signs, use_loss_decoding, use_independent_decoder, use_independent_and_first_comb_decoder, simulate_data, noise_params)\u001b[0m\n\u001b[1;32m    601\u001b[0m                     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                     \u001b[0mreturn_matrix_with_observables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'MLE'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m                     \u001b[0mfinal_dem_hyperedges_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservables_errors_interactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLE_Loss_Decoder_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_dem_loss_mle_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeasurement_event\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_matrix_with_observables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_matrix_with_observables\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# final_dem_hyperedges_matrix doesn't contain observables, only detectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m                     \u001b[0;31m# print(f'Total loss decoder time per shot is {time.time() - start_time:.4f}s.')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local_erasure_biased_errors/BiasedErasure/delayed_erasure_decoders/MLE_Loss_Decoder.py\u001b[0m in \u001b[0;36mgenerate_dem_loss_mle_experiment\u001b[0;34m(self, measurement_event, return_matrix_with_observables)\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m11\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_type\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m  \u001b[0;34m'independent'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Independent decoder:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0mstart_time_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                 \u001b[0mfinal_dem_hyperedges_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_all_DEMs_and_sum_over_independent_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_hyperedges_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m                 \u001b[0;31m# print(f'Summing over all relevant DEMs to generate the final DEM took {time.time() - start_time_ind:.5f}s')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local_erasure_biased_errors/BiasedErasure/delayed_erasure_decoders/MLE_Loss_Decoder.py\u001b[0m in \u001b[0;36mgenerate_all_DEMs_and_sum_over_independent_events\u001b[0;34m(self, use_pre_processed_data, return_hyperedges_matrix, remove_gates_due_to_loss)\u001b[0m\n\u001b[1;32m   1057\u001b[0m         \u001b[0;31m# sum over all loss DEMs:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m         \u001b[0mfinal_hyperedges_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_DEMs_high_order_csr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEMs_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEMs_loss_pauli_events\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_detectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_detectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProbs_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mProbs_loss_pauli_events\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# GB: new Probs_loss_pauli_events. TODO: change this function to also get Probs_loss_pauli_events\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m         \u001b[0;31m# print(f'New method: Time to sum over all DEMS (independent, combination, Pauli) with high order equation: {time.time() - start_time:.4f}s.')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local_erasure_biased_errors/BiasedErasure/delayed_erasure_decoders/MLE_Loss_Decoder.py\u001b[0m in \u001b[0;36mcombine_DEMs_high_order_csr\u001b[0;34m(self, DEMs_list, num_detectors, Probs_list)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[0mfinal_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpattern_to_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1542\u001b[0;31m             \u001b[0mprob_i_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1543\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1544\u001b[0m                 \u001b[0;31m# Consider terms where 3 specific events happen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local_erasure_biased_errors/BiasedErasure/delayed_erasure_decoders/MLE_Loss_Decoder.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[0mfinal_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpattern_to_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1542\u001b[0;31m             \u001b[0mprob_i_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1543\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1544\u001b[0m                 \u001b[0;31m# Consider terms where 3 specific events happen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   3086\u001b[0m     \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3087\u001b[0m     \"\"\"\n\u001b[0;32m-> 3088\u001b[0;31m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0m\u001b[1;32m   3089\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   3090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     passkwargs = {k: v for k, v in kwargs.items()\n\u001b[0m\u001b[1;32m     71\u001b[0m                   if v is not np._NoValue}\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     passkwargs = {k: v for k, v in kwargs.items()\n\u001b[0m\u001b[1;32m     71\u001b[0m                   if v is not np._NoValue}\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_trace_dispatch_regular.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0madditional_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0mpydev_step_cmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madditional_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydev_step_cmd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m             \u001b[0mis_stepping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydev_step_cmd\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydb_disposed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from BiasedErasure.delayed_erasure_decoders.Experimental_Loss_Decoder import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "num_rounds = 3\n",
    "num_cx = 3\n",
    "distance = 5\n",
    "decoder_basis = 'XX'\n",
    "gate_ordering = ['N', 'Z']\n",
    "exp_measurements = np.load('2024_10_15_measurement_events_1CNOT_XX.npy')#[:100, :]\n",
    "exp_measurements = np.concatenate([exp_measurements[:, 0, :distance**2-1],\n",
    "                                exp_measurements[:, 1, :distance**2-1],\n",
    "                                exp_measurements[:, 0, distance**2-1:2*(distance**2-1)],\n",
    "                                exp_measurements[:, 1, distance**2-1:2*(distance**2-1)],\n",
    "                                exp_measurements[:, 0, 2*(distance**2-1):],\n",
    "                                exp_measurements[:, 1, 2*(distance**2-1):]], axis=1)\n",
    "\n",
    "noise_params = {'idle_loss_rate': 2.793300220405646e-07, 'idle_error_rate': np.array([6.60547942e-09, 3.38336163e-08, 2.67533789e-07]),\n",
    "                    'entangling_zone_error_rate': np.array([3.66476387e-04, 6.14732819e-06, 2.35857048e-03]),\n",
    "                    'entangling_gate_error_rate': [2.2260729018707513e-05, 0.00017139584089578063, 0.0012948317242757047, 2.2260729018707513e-05, 0, 0, 0, 0.00017139584089578063, 0, 0, 0, 0.0012948317242757047, 0, 0, 0.002621736717313752],\n",
    "                    'entangling_gate_loss_rate': 0.00039272255674060926, 'single_qubit_error_rate': np.array([1.53681034e-05, 9.93583065e-04, 1.94650113e-05]),\n",
    "                    'reset_error_rate': 5.89409983290463e-05, 'measurement_error_rate': 0.0006138700821647161, 'reset_loss_rate': 0.0007531131027610011, 'measurement_loss_rate': 0.07131074481520218, 'ancilla_idle_loss_rate': 1.6989311035347498e-07,\n",
    "                    'ancilla_idle_error_rate': np.array([1.46727589e-07, 4.60893305e-08, 2.30298714e-06]), 'ancilla_reset_error_rate': 0.024549181355318986, 'ancilla_measurement_error_rate': 0.0012815874700447462, 'ancilla_reset_loss_rate': 0.00019528486460263086, 'ancilla_measurement_loss_rate': 0.00047357577582906143,\n",
    "                    'gate_noise': LogicalCircuit.ancilla_data_differentiated_gate_noise, 'idle_noise': LogicalCircuit.ancilla_data_differentiated_idle_noise}\n",
    "\n",
    "Meta_params = {'architecture': 'CBQC', 'code': 'Rotated_Surface', 'logical_basis': decoder_basis,\n",
    "            'bias_preserving_gates': 'False',\n",
    "            'noise': 'atom_array', 'is_erasure_biased': 'False', 'LD_freq': '1000', 'LD_method': 'None',\n",
    "            'SSR': 'True', 'cycles': str(num_rounds - 1),\n",
    "            'ordering': gate_ordering,\n",
    "            'decoder': 'MLE',\n",
    "            'circuit_type': f'logical_CX_NL{num_rounds}_NCX{num_cx}', 'Steane_type': 'None', 'printing': 'False', 'num_logicals': '2',\n",
    "            'loss_decoder': 'independent',\n",
    "            'obs_pos': 'd-1', 'n_r': '0'}\n",
    "\n",
    "simulate_data = True\n",
    "\n",
    "if simulate_data:\n",
    "    detection_events_signs = None\n",
    "    exp_measurements = None\n",
    "    num_shots = 1000\n",
    "\n",
    "else:\n",
    "    # Load the theory circuit\n",
    "    _, _, _, circuit = get_simulated_measurement_events(Meta_params, distance, distance, 1, noise_params)\n",
    "    # Use the theory circuit to get the detection events and observable flips corresponding to the exp data\n",
    "    detection_events, observable_flips = circuit.compile_m2d_converter().convert(measurements=exp_measurements.astype(bool), separate_observables=True)\n",
    "    # Find detection event signs\n",
    "    detection_events_signs = -np.sign(2*np.nanmean(detection_events.astype(int), axis=0)-1).astype(int)\n",
    "    exp_measurements = np.load(f'/Users/gefenbaranes/Documents/2024_10_15_measurement_events_1CNOT_XX.npy')\n",
    "    exp_measurements = np.concatenate([exp_measurements[:, 0, :distance**2-1],\n",
    "                                    exp_measurements[:, 1, :distance**2-1],\n",
    "                                    exp_measurements[:, 0, distance**2-1:2*(distance**2-1)],\n",
    "                                    exp_measurements[:, 1, distance**2-1:2*(distance**2-1)],\n",
    "                                    exp_measurements[:, 0, 2*(distance**2-1):],\n",
    "                                    exp_measurements[:, 1, 2*(distance**2-1):]], axis=1)\n",
    "\n",
    "\n",
    "# Now let's decode!\n",
    "use_loss_decoding = True  # if False: use same DEM every shot, without utilizing SSR.\n",
    "use_independent_decoder = True  # if False: in every lifecycle, we just apply supercheck at the end. If True: we count the full lifecycle with different potential loss locations and corresponding Clifford propagations.\n",
    "use_independent_and_first_comb_decoder = False  # This is relevant only if use_independent_decoder=True. If False: use only independent lifecycles. If True: adds a single combination of lifecycles to the decoder.\n",
    "output_dir = '.'\n",
    "# DO IT\n",
    "predictions, observable_flips, dems_list = Loss_MLE_Decoder_Experiment(Meta_params, distance, distance, output_dir,\n",
    "                                                                    exp_measurements,\n",
    "                                                                    detection_events_signs, use_loss_decoding,\n",
    "                                                                    use_independent_decoder,\n",
    "                                                                    use_independent_and_first_comb_decoder,\n",
    "                                                                    simulate_data=simulate_data, logical_gaps=False,\n",
    "                                                                    noise_params=noise_params, num_shots=num_shots)\n",
    "logical_probability = np.mean(np.logical_xor(observable_flips, predictions))\n",
    "print('/n logical error', logical_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9299999999999999"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-0.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.954"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-logical_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.956"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-logical_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Meta_params['printing'] = 'False'\n",
    "\n",
    "_, _, _, circuit = get_simulated_measurement_events(Meta_params, distance, distance, 1, noise_params)\n",
    "print(circuit.num_detectors)\n",
    "print(\"HI\")\n",
    "print(circuit.detector_error_model(approximate_disjoint_errors=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a plot of results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_rounds = 3\n",
    "num_cxs_per_round = np.array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21])\n",
    "logical_errors = np.array([0.16, 0.202, 0.234, 0.292, 0.322, 0.35, 0.384, 0.393, 0.4171, 0.436, 0.4517])\n",
    "errors_per_rounds = .5*(1-(1-logical_errors/.5)**(1/(num_rounds*num_cxs_per_round)))\n",
    "errors_per_rounds_division = logical_errors / (num_rounds*num_cxs_per_round)\n",
    "plt.plot(num_cxs_per_round, errors_per_rounds, marker='o', color='blue', label='fancy formula')\n",
    "plt.plot(num_cxs_per_round, errors_per_rounds_division, marker='o', color='red', label='divide by # rounds')\n",
    "plt.ylabel('Error per CNOT')\n",
    "plt.xlabel('Number of CNOTs per round')\n",
    "plt.xticks(num_cxs_per_round)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layers = 3\n",
      "num_CX_per_layer = 1\n",
      "final measurement_index = 146\n",
      "Preprocessing is done! it took 59.61s\n",
      "0 [0.025]\n",
      "infidelity 0.975\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from BiasedErasure.delayed_erasure_decoders.Experimental_Loss_Decoder import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "num_cxs = np.array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21])\n",
    "num_cxs = np.array([1])\n",
    "errors = np.array([0.16, 0.202, 0.234, 0.292, 0.322, 0.35, 0.384, 0.393, 0.4171, 0.429, 0.47])\n",
    "num_rounds = 3\n",
    "distance = 5\n",
    "decoder_basis = 'XX'\n",
    "gate_ordering = ['N', 'Z']\n",
    "\n",
    "\n",
    "errors = []\n",
    "for num_cx in num_cxs:\n",
    "    noise_params = {'idle_loss_rate': 2.793300220405646e-07, 'idle_error_rate': np.array([6.60547942e-09, 3.38336163e-08, 2.67533789e-07]),\n",
    "                    'entangling_zone_error_rate': np.array([3.66476387e-04, 6.14732819e-06, 2.35857048e-03]),\n",
    "                    'entangling_gate_error_rate': [2.2260729018707513e-05, 0.00017139584089578063, 0.0012948317242757047, 2.2260729018707513e-05, 0, 0, 0, 0.00017139584089578063, 0, 0, 0, 0.0012948317242757047, 0, 0, 0.002621736717313752],\n",
    "                    'entangling_gate_loss_rate': 0.00039272255674060926, 'single_qubit_error_rate': np.array([1.53681034e-05, 9.93583065e-04, 1.94650113e-05]),\n",
    "                    'reset_error_rate': 5.89409983290463e-05, 'measurement_error_rate': 0.0006138700821647161, 'reset_loss_rate': 0.0007531131027610011, 'measurement_loss_rate': 0.07131074481520218, 'ancilla_idle_loss_rate': 1.6989311035347498e-07,\n",
    "                    'ancilla_idle_error_rate': np.array([1.46727589e-07, 4.60893305e-08, 2.30298714e-06]), 'ancilla_reset_error_rate': 0.024549181355318986, 'ancilla_measurement_error_rate': 0.0012815874700447462, 'ancilla_reset_loss_rate': 0.00019528486460263086, 'ancilla_measurement_loss_rate': 0.00047357577582906143,\n",
    "                    'gate_noise': LogicalCircuit.ancilla_data_differentiated_gate_noise, 'idle_noise': LogicalCircuit.ancilla_data_differentiated_idle_noise}\n",
    "\n",
    "    Meta_params = {'architecture': 'CBQC', 'code': 'Rotated_Surface', 'logical_basis': decoder_basis,\n",
    "                'bias_preserving_gates': 'False',\n",
    "                'noise': 'atom_array', 'is_erasure_biased': 'False', 'LD_freq': '1000', 'LD_method': 'None',\n",
    "                'SSR': 'True', 'cycles': str(num_rounds - 1),\n",
    "                'ordering': gate_ordering,\n",
    "                'decoder': 'MLE',\n",
    "                'circuit_type': f'logical_CX_NL{num_rounds}_NCX{num_cx}', 'Steane_type': 'None', 'printing': 'False', 'num_logicals': '2',\n",
    "                'loss_decoder': 'independent',\n",
    "                'obs_pos': 'd-1', 'n_r': '0'}\n",
    "\n",
    "\n",
    "    # Load the experimental measurements\n",
    "    exp_measurements = np.load('2024_10_15_measurement_events_1CNOT_XX.npy')#[:100, :]\n",
    "    exp_measurements = np.concatenate([exp_measurements[:, 0, :distance**2-1],\n",
    "                                       exp_measurements[:, 1, :distance**2-1],\n",
    "                                       exp_measurements[:, 0, distance**2-1:2*(distance**2-1)],\n",
    "                                       exp_measurements[:, 1, distance**2-1:2*(distance**2-1)],\n",
    "                                       exp_measurements[:, 0, 2*(distance**2-1):],\n",
    "                                       exp_measurements[:, 1, 2*(distance**2-1):]], axis=1)\n",
    "\n",
    "    # Load the theory circuit\n",
    "    # theory_measurements, theory_detectors, theory_observables, circuit = get_simulated_measurement_events(Meta_params, distance, distance, 1000, noise_params)\n",
    "    # print(2 in theory_measurements)\n",
    "    # Use the theory circuit to get the detection events and observable flips corresponding to the exp data\n",
    "    # exp_detectors, exp_observables = circuit.compile_m2d_converter().convert(measurements=exp_measurements.astype(bool), separate_observables=True)\n",
    "    # Find detection event signs\n",
    "    # exp_detection_events_signs = -np.sign(2*np.nanmean(exp_detectors.astype(int), axis=0)-1).astype(int)\n",
    "\n",
    "    # Now let's decode!\n",
    "    use_loss_decoding = True  # if False: use same DEM every shot, without utilizing SSR.\n",
    "    use_independent_decoder = True  # if False: in every lifecycle, we just apply supercheck at the end. If True: we count the full lifecycle with different potential loss locations and corresponding Clifford propagations.\n",
    "    use_independent_and_first_comb_decoder = False  # This is relevant only if use_independent_decoder=True. If False: use only independent lifecycles. If True: adds a single combination of lifecycles to the decoder.\n",
    "    output_dir = '.'\n",
    "    simulate_data = True\n",
    "    num_shots = 1000\n",
    "    # DO IT\n",
    "    \"\"\"exp_predictions, exp_observable_flips, dems_list = Loss_MLE_Decoder_Experiment(Meta_params, distance, distance, output_dir,\n",
    "                                                                        exp_measurements,\n",
    "                                                                        exp_detection_events_signs, use_loss_decoding,\n",
    "                                                                        use_independent_decoder,\n",
    "                                                                        use_independent_and_first_comb_decoder,\n",
    "                                                                        simulate_data=simulate_data, logical_gaps=False,\n",
    "                                                                        noise_params=noise_params)\n",
    "    \n",
    "    exp_logical_probability = np.mean(np.logical_xor(exp_observable_flips, exp_predictions))\"\"\"\n",
    "\n",
    "    #print('infidelity', 1-exp_logical_probability)\n",
    "    theory_predictions, theory_observable_flips, dems_list = Loss_MLE_Decoder_Experiment(Meta_params, distance, distance, output_dir,\n",
    "                                                                        None,\n",
    "                                                                        None, use_loss_decoding,\n",
    "                                                                        use_independent_decoder,\n",
    "                                                                        use_independent_and_first_comb_decoder,\n",
    "                                                                        simulate_data=simulate_data, logical_gaps=False,\n",
    "                                                                        noise_params=noise_params, num_shots = num_shots)\n",
    "\n",
    "    theory_logical_probability = np.mean(np.logical_xor(theory_observable_flips, theory_predictions))\n",
    "    errors.append(theory_logical_probability)\n",
    "    print(errors)\n",
    "    print('infidelity', 1-theory_logical_probability)\n",
    "\n",
    "\n",
    "plt.plot(num_cxs, errors, marker='o', color='blue')\n",
    "plt.ylabel('Error per CNOT')\n",
    "plt.xlabel('Number of CNOTs per round')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taking data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layers = 3\n",
      "num_CX_per_layer = 1\n",
      "final measurement_index = 146\n",
      "Preprocessing is done! it took 60.88s\n",
      "0 1000 2000 3000 4000 for num_layers = 3, num_cxs_per_round = 1 we get logical error 0.0222 +- 0.002083610328252382\n",
      "\n",
      "num_layers = 3\n",
      "num_CX_per_layer = 3\n",
      "final measurement_index = 146\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kh/9b4nnp7x0s7c1fp6wn1qhrnm0000gn/T/ipykernel_10161/761206627.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0muse_independent_and_first_comb_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# This is relevant only if use_independent_decoder=True. If False: use only independent lifecycles. If True: adds a single combination of lifecycles to the decoder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/gefenbaranes/Documents/CX_experiment'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         predictions, observable_flips, dems_list = Loss_MLE_Decoder_Experiment(Meta_params, distance, distance, output_dir,\n\u001b[0m\u001b[1;32m     71\u001b[0m                                                                             \u001b[0mmeasurement_events\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                                                                             \u001b[0mdetection_events_signs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_loss_decoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local_erasure_biased_errors/BiasedErasure/delayed_erasure_decoders/Experimental_Loss_Decoder.py\u001b[0m in \u001b[0;36mLoss_MLE_Decoder_Experiment\u001b[0;34m(Meta_params, dx, dy, output_dir, measurement_events, detection_events_signs, use_loss_decoding, use_independent_decoder, use_independent_and_first_comb_decoder, simulate_data, first_comb_weight, noise_params, logical_gaps, num_shots)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlogical_gaps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# Step 1 - decode:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 predictions, observable_flips, dems_list = simulator.count_logical_errors_experiment(num_shots = num_shots, dx = dx, dy = dy,\n\u001b[0m\u001b[1;32m     44\u001b[0m                                                         \u001b[0mmeasurement_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeasurement_events\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_events_signs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdetection_events_signs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                                                         \u001b[0muse_loss_decoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_loss_decoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local_erasure_biased_errors/BiasedErasure/main_code/Simulator.py\u001b[0m in \u001b[0;36mcount_logical_errors_experiment\u001b[0;34m(self, num_shots, dx, dy, measurement_events, detection_events_signs, use_loss_decoding, use_independent_decoder, use_independent_and_first_comb_decoder, simulate_data, noise_params)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0mMLE_Loss_Decoder_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_loss_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this part can be improved to be a bit faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprinting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Decoder initialized, it took {time.time() - start_time:.2f}s for everything'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local_erasure_biased_errors/BiasedErasure/delayed_erasure_decoders/MLE_Loss_Decoder.py\u001b[0m in \u001b[0;36minitialize_loss_decoder\u001b[0;34m(self, **kargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_circuit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_filename_dems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Preprocessing is done! it took {time.time() - start_time:.2f}s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local_erasure_biased_errors/BiasedErasure/delayed_erasure_decoders/MLE_Loss_Decoder.py\u001b[0m in \u001b[0;36mpreprocess_circuit\u001b[0;34m(self, full_filename)\u001b[0m\n\u001b[1;32m    884\u001b[0m                         \u001b[0;31m#     num_potential_losses += len(losses_indices_in_round)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m                         \u001b[0mhyperedges_matrix_dem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_dem_loss_circuit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_by_instruction_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses_by_instruction_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_filename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# GB: change event prob to 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m                         \u001b[0;31m# save into the file:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_unique_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_by_instruction_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local_erasure_biased_errors/BiasedErasure/delayed_erasure_decoders/MLE_Loss_Decoder.py\u001b[0m in \u001b[0;36mgenerate_dem_loss_circuit\u001b[0;34m(self, losses_by_instruction_ix, event_probability, full_filename, remove_gates_due_to_loss)\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m             \u001b[0;31m# replace final observables with detectors:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1851\u001b[0;31m             \u001b[0mfinal_loss_circuit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservables_to_detectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_circuit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m             \u001b[0;31m# get the dem (with observables on columns):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local_erasure_biased_errors/BiasedErasure/delayed_erasure_decoders/MLE_Loss_Decoder.py\u001b[0m in \u001b[0;36mobservables_to_detectors\u001b[0;34m(self, circuit)\u001b[0m\n\u001b[1;32m   1374\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstruction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from BiasedErasure.delayed_erasure_decoders.Experimental_Loss_Decoder import *\n",
    "import numpy as np\n",
    "# num_rounds = 5\n",
    "\n",
    "P_dict = {}\n",
    "P_err_dict = {}\n",
    "num_shots_dict = {}\n",
    "num_errors_dict = {}\n",
    "\n",
    "num_layers_vec = [3,2]\n",
    "num_cxs_per_round_vec = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]\n",
    "for num_layers in num_layers_vec:\n",
    "    num_shots_dict[num_layers] = {}\n",
    "    num_errors_dict[num_layers] = {}\n",
    "    P_dict[num_layers] = {}\n",
    "    P_err_dict[num_layers] = {}\n",
    "    for num_cxs_per_round in num_cxs_per_round_vec:\n",
    "        distance = 5\n",
    "        decoder_basis = 'XX'\n",
    "        gate_ordering = ['N', 'Z']\n",
    "        noise_params = {'idle_loss_rate': 2.793300220405646e-07, 'idle_error_rate': np.array([6.60547942e-09, 3.38336163e-08, 2.67533789e-07]),\n",
    "                        'entangling_zone_error_rate': np.array([3.66476387e-04, 6.14732819e-06, 2.35857048e-03]),\n",
    "                        'entangling_gate_error_rate': [2.2260729018707513e-05, 0.00017139584089578063, 0.0012948317242757047, 2.2260729018707513e-05, 0, 0, 0, 0.00017139584089578063, 0, 0, 0, 0.0012948317242757047, 0, 0, 0.002621736717313752],\n",
    "                        'entangling_gate_loss_rate': 0.00039272255674060926, 'single_qubit_error_rate': np.array([1.53681034e-05, 9.93583065e-04, 1.94650113e-05]),\n",
    "                        'reset_error_rate': 5.89409983290463e-05, 'measurement_error_rate': 0.0006138700821647161, 'reset_loss_rate': 0.0007531131027610011, 'measurement_loss_rate': 0.07131074481520218, 'ancilla_idle_loss_rate': 1.6989311035347498e-07,\n",
    "                        'ancilla_idle_error_rate': np.array([1.46727589e-07, 4.60893305e-08, 2.30298714e-06]), 'ancilla_reset_error_rate': 0.024549181355318986, 'ancilla_measurement_error_rate': 0.0012815874700447462, 'ancilla_reset_loss_rate': 0.00019528486460263086, 'ancilla_measurement_loss_rate': 0.00047357577582906143,\n",
    "                        'gate_noise': LogicalCircuit.ancilla_data_differentiated_gate_noise, 'idle_noise': LogicalCircuit.ancilla_data_differentiated_idle_noise}\n",
    "\n",
    "\n",
    "\n",
    "        Meta_params = {'architecture': 'CBQC', 'code': 'Rotated_Surface', 'logical_basis': decoder_basis,\n",
    "                    'bias_preserving_gates': 'False',\n",
    "                    'noise': 'atom_array', 'is_erasure_biased': 'False', 'LD_freq': '1000', 'LD_method': 'None',\n",
    "                    'SSR': 'True', 'cycles': str(num_layers - 1),\n",
    "                    'ordering': gate_ordering,\n",
    "                    'decoder': 'MLE',\n",
    "                    'circuit_type': f'logical_CX_NL{num_layers}_NCX{num_cxs_per_round}', 'Steane_type': 'None', 'printing': 'False', 'num_logicals': '2',\n",
    "                    'loss_decoder': 'independent',\n",
    "                    'obs_pos': 'd-1', 'n_r': '0'}\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        simulate_data = True\n",
    "\n",
    "        if simulate_data:\n",
    "            detection_events_signs = None\n",
    "            measurement_events = None\n",
    "            num_shots = 5000\n",
    "\n",
    "        else:\n",
    "            # Load the experimental measurements\n",
    "            exp_measurements = np.load(f'/Users/gefenbaranes/Documents/2024_10_15_measurement_events_1CNOT_XX.npy')\n",
    "            exp_measurements = np.concatenate([exp_measurements[:, 0, :distance**2-1],\n",
    "                                            exp_measurements[:, 1, :distance**2-1],\n",
    "                                            exp_measurements[:, 0, distance**2-1:2*(distance**2-1)],\n",
    "                                            exp_measurements[:, 1, distance**2-1:2*(distance**2-1)],\n",
    "                                            exp_measurements[:, 0, 2*(distance**2-1):],\n",
    "                                            exp_measurements[:, 1, 2*(distance**2-1):]], axis=1)\n",
    "            measurement_events = exp_measurements\n",
    "            # Load the theory circuit\n",
    "            _, _, _, circuit = get_simulated_measurement_events(Meta_params, distance, distance, 1, noise_params)\n",
    "            # Use the theory circuit to get the detection events and observable flips corresponding to the exp data\n",
    "            detection_events, observable_flips = circuit.compile_m2d_converter().convert(measurements=exp_measurements.astype(bool), separate_observables=True)\n",
    "            # Find detection event signs\n",
    "            detection_events_signs = -np.sign(2*np.nanmean(detection_events.astype(int), axis=0)-1).astype(int)\n",
    "\n",
    "        # Now let's decode!\n",
    "        use_loss_decoding = True  # if False: use same DEM every shot, without utilizing SSR.\n",
    "        use_independent_decoder = True  # if False: in every lifecycle, we just apply supercheck at the end. If True: we count the full lifecycle with different potential loss locations and corresponding Clifford propagations.\n",
    "        use_independent_and_first_comb_decoder = False  # This is relevant only if use_independent_decoder=True. If False: use only independent lifecycles. If True: adds a single combination of lifecycles to the decoder.\n",
    "        output_dir = '/Users/gefenbaranes/Documents/CX_experiment'\n",
    "        predictions, observable_flips, dems_list = Loss_MLE_Decoder_Experiment(Meta_params, distance, distance, output_dir,\n",
    "                                                                            measurement_events,\n",
    "                                                                            detection_events_signs, use_loss_decoding,\n",
    "                                                                            use_independent_decoder,\n",
    "                                                                            use_independent_and_first_comb_decoder,\n",
    "                                                                            simulate_data=simulate_data, logical_gaps=False,\n",
    "                                                                            noise_params=noise_params, num_shots=num_shots)\n",
    "        logical_probability = np.mean(np.logical_xor(observable_flips, predictions))\n",
    "        num_errors = np.sum(np.logical_xor(observable_flips, predictions))\n",
    "        logical_probability_error = (np.sqrt(logical_probability*(1-logical_probability)/num_shots))\n",
    "        print(f'for num_layers = {num_layers}, num_cxs_per_round = {num_cxs_per_round} we get logical error {logical_probability} +- {logical_probability_error}\\n')\n",
    "\n",
    "        \n",
    "        P_dict[num_layers][num_cxs_per_round] = logical_probability\n",
    "        P_err_dict[num_layers][num_cxs_per_round] = logical_probability_error\n",
    "        num_shots_dict[num_layers][num_cxs_per_round] = num_shots\n",
    "        num_errors_dict[num_layers][num_cxs_per_round] = num_errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogicalCircuit' object has no attribute 'Pauli_DEM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kh/9b4nnp7x0s7c1fp6wn1qhrnm0000gn/T/ipykernel_68890/1495820122.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcircuit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPauli_DEM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'LogicalCircuit' object has no attribute 'Pauli_DEM'"
     ]
    }
   ],
   "source": [
    "print(circuit.Pauli_DEM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with H = Ry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layers = 3\n",
      "num_CX_per_layer = 3\n",
      "final measurement_index = 146\n",
      "[ 1  1 -1 -1  1  1  1  1 -1 -1  1  1 -1 -1  1 -1  1  1  1 -1  1  1 -1 -1\n",
      "  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1 -1  1  1  1 -1  1 -1 -1\n",
      "  1  1 -1 -1  1  1  1  1 -1 -1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1  1  1\n",
      "  1  1 -1 -1  1  1  1  1 -1 -1 -1 -1 -1 -1  1  1 -1 -1 -1 -1  1  1 -1 -1]\n",
      "[ 1.  1. -1. -1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1. -1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1. -1.  1. -1. -1.  1. -1. -1.  1.\n",
      " -1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1.  1.\n",
      "  1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1. -1. -1.  1. -1.\n",
      "  1.  1. -1. -1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1.  1.  1. -1. -1.\n",
      " -1. -1.  1.  1. -1. -1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,\n",
       "        2., -2.,  2.,  0.,  0.,  0.,  0., -2., -2.,  2.,  2., -2., -2.,\n",
       "        2.,  0.,  0.,  2.,  0.,  0.,  2.,  0.,  0.,  2.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0., -2., -2.,  0., -2.,  0.,  2.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -2., -2.,  0.,  2.,  0.,\n",
       "        0.,  2.,  2.,  0.,  0.,  0., -2.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_measurements = np.load(f'/Users/gefenbaranes/Documents/2024_10_15_measurement_events_1CNOT_XX.npy')\n",
    "exp_measurements = np.concatenate([exp_measurements[:, 0, :distance**2-1],\n",
    "                                exp_measurements[:, 1, :distance**2-1],\n",
    "                                exp_measurements[:, 0, distance**2-1:2*(distance**2-1)],\n",
    "                                exp_measurements[:, 1, distance**2-1:2*(distance**2-1)],\n",
    "                                exp_measurements[:, 0, 2*(distance**2-1):],\n",
    "                                exp_measurements[:, 1, 2*(distance**2-1):]], axis=1)\n",
    "_, _, _, circuit = get_simulated_measurement_events(Meta_params, distance, distance, 1, noise_params)\n",
    "detection_events, observable_flips = circuit.compile_m2d_converter().convert(measurements=exp_measurements.astype(bool), separate_observables=True)\n",
    "detection_events_signs_theory = -np.sign(2*np.nanmean(detection_events.astype(int), axis=0)-1).astype(int)\n",
    "print(detection_events_signs_theory)\n",
    "\n",
    "detection_events_signs_exp =  np.array([ 1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,\n",
    "        1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,\n",
    "        1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,\n",
    "        1., -1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,\n",
    "        1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,\n",
    "       -1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,\n",
    "        1.,  1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1.,\n",
    "       -1.,  1.,  1., -1., -1.])\n",
    "\n",
    "print(detection_events_signs_exp)\n",
    "\n",
    "\n",
    "detection_events_signs_exp - detection_events_signs_theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with H = Rydagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layers = 3\n",
      "num_CX_per_layer = 3\n",
      "final measurement_index = 146\n",
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1 -1  1  1  1  1\n",
      "  1  1  1  1 -1 -1 -1 -1  1  1 -1 -1 -1 -1  1  1 -1  1  1  1 -1  1 -1 -1\n",
      "  1  1 -1 -1  1  1  1  1 -1 -1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1  1  1 -1 -1 -1 -1  1  1 -1 -1]\n",
      "[ 1.  1. -1. -1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1. -1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1. -1.  1. -1. -1.  1. -1. -1.  1.\n",
      " -1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1.  1.\n",
      "  1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1. -1. -1.  1. -1.\n",
      "  1.  1. -1. -1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1.  1.  1. -1. -1.\n",
      " -1. -1.  1.  1. -1. -1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0., -2., -2.,  0.,  0.,  0.,  0., -2., -2.,  0.,  0.,  0.,\n",
       "        0., -2.,  2.,  0.,  0.,  0.,  0., -2., -2.,  0.,  0., -2., -2.,\n",
       "        0., -2.,  0.,  2.,  0.,  0.,  0., -2.,  0.,  2.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0., -2., -2.,  0., -2.,  0.,  2.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,\n",
       "        0.,  2.,  2.,  0.,  0.,  2.,  0.,  0.,  0., -2., -2.,  0.,  0.,\n",
       "        0.,  0., -2., -2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_measurements = np.load(f'/Users/gefenbaranes/Documents/2024_10_15_measurement_events_1CNOT_XX.npy')\n",
    "exp_measurements = np.concatenate([exp_measurements[:, 0, :distance**2-1],\n",
    "                                exp_measurements[:, 1, :distance**2-1],\n",
    "                                exp_measurements[:, 0, distance**2-1:2*(distance**2-1)],\n",
    "                                exp_measurements[:, 1, distance**2-1:2*(distance**2-1)],\n",
    "                                exp_measurements[:, 0, 2*(distance**2-1):],\n",
    "                                exp_measurements[:, 1, 2*(distance**2-1):]], axis=1)\n",
    "_, _, _, circuit = get_simulated_measurement_events(Meta_params, distance, distance, 1, noise_params)\n",
    "detection_events, observable_flips = circuit.compile_m2d_converter().convert(measurements=exp_measurements.astype(bool), separate_observables=True)\n",
    "detection_events_signs_theory = -np.sign(2*np.nanmean(detection_events.astype(int), axis=0)-1).astype(int)\n",
    "print(detection_events_signs_theory)\n",
    "\n",
    "detection_events_signs_exp =  np.array([ 1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,\n",
    "        1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,\n",
    "        1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,\n",
    "        1., -1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,\n",
    "        1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,\n",
    "       -1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,\n",
    "        1.,  1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1.,\n",
    "       -1.,  1.,  1., -1., -1.])\n",
    "\n",
    "print(detection_events_signs_exp)\n",
    "\n",
    "\n",
    "detection_events_signs_exp - detection_events_signs_theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1. -1. -1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1. -1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1. -1.  1. -1. -1.  1. -1. -1.  1.\n",
      " -1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1.  1.\n",
      "  1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1. -1. -1.  1. -1.\n",
      "  1.  1. -1. -1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1.  1.  1. -1. -1.\n",
      " -1. -1.  1.  1. -1. -1.]\n",
      "num_CX_per_layer = 3\n",
      "final measurement_index = 146\n",
      "[ 1  1 -1 -1  1  1  1  1 -1 -1  1  1 -1 -1  1 -1  1  1  1 -1  1  1 -1 -1\n",
      "  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1 -1  1  1  1 -1  1 -1 -1\n",
      "  1  1 -1 -1  1  1  1  1 -1 -1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1  1  1\n",
      "  1  1 -1 -1  1  1  1  1 -1 -1 -1 -1 -1 -1  1  1 -1 -1 -1 -1  1  1 -1 -1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -2.,\n",
       "       -2.,  2., -2.,  0.,  0.,  0.,  0.,  2.,  2., -2., -2.,  2.,  2.,\n",
       "       -2.,  0.,  0., -2.,  0.,  0., -2.,  0.,  0., -2.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  2.,  2.,  0.,  2.,  0., -2.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  2.,  0., -2.,  0.,\n",
       "        0., -2., -2.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_events_signs_exp = np.array([ 1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,\n",
    "        1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,\n",
    "        1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,\n",
    "        1., -1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,\n",
    "        1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,\n",
    "       -1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,\n",
    "        1.,  1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1.,\n",
    "       -1.,  1.,  1., -1., -1.])\n",
    "\n",
    "print(detection_events_signs_exp)\n",
    "exp_measurements = np.load(f'/Users/gefenbaranes/Documents/2024_10_15_measurement_events_1CNOT_XX.npy')\n",
    "exp_measurements = np.concatenate([exp_measurements[:, 0, :distance**2-1],\n",
    "                                exp_measurements[:, 1, :distance**2-1],\n",
    "                                exp_measurements[:, 0, distance**2-1:2*(distance**2-1)],\n",
    "                                exp_measurements[:, 1, distance**2-1:2*(distance**2-1)],\n",
    "                                exp_measurements[:, 0, 2*(distance**2-1):],\n",
    "                                exp_measurements[:, 1, 2*(distance**2-1):]], axis=1)\n",
    "\n",
    "\n",
    "_, _, _, circuit = get_simulated_measurement_events(Meta_params, distance, distance, 1, noise_params)\n",
    "# Use the theory circuit to get the detection events and observable flips corresponding to the exp data\n",
    "detection_events, observable_flips = circuit.compile_m2d_converter().convert(measurements=exp_measurements.astype(bool), separate_observables=True)\n",
    "# Find detection event signs\n",
    "detection_events_signs_theory = -np.sign(2*np.nanmean(detection_events.astype(int), axis=0)-1).astype(int)\n",
    "print(detection_events_signs_theory)\n",
    "\n",
    "\n",
    "detection_events_signs_theory - detection_events_signs_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_CX_per_layer = 3\n",
      "final measurement_index = 146\n"
     ]
    }
   ],
   "source": [
    "num_shots = 100\n",
    "# Change #2: a function that doesn't decode, just gives you the measurements, detection events, observables and the circuit:\n",
    "measurement_events_all_shots, detection_events_all_shots, observable_flips_all_shots, LogicalCircuit = get_simulated_measurement_events(Meta_params, distance, distance, num_shots, noise_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1 -1 -1  1  1  1  1 -1 -1  1  1 -1 -1  1 -1  1  1  1 -1  1  1 -1 -1\n",
      "  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1 -1  1  1  1 -1  1 -1 -1\n",
      "  1  1 -1 -1  1  1  1  1 -1 -1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1  1  1\n",
      "  1  1 -1 -1  1  1  1  1 -1 -1 -1 -1 -1 -1  1  1 -1 -1 -1 -1  1  1 -1 -1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "exp_measurements = np.load(f'/Users/gefenbaranes/Documents/2024_10_15_measurement_events_1CNOT_XX.npy')\n",
    "measurement_events = np.concatenate([exp_measurements[:, 0, :distance**2-1],\n",
    "                            exp_measurements[:, 1, :distance**2-1],\n",
    "                            exp_measurements[:, 0, distance**2-1:2*(distance**2-1)],\n",
    "                            exp_measurements[:, 1, distance**2-1:2*(distance**2-1)],\n",
    "                            exp_measurements[:, 0, 2*(distance**2-1):],\n",
    "                            exp_measurements[:, 1, 2*(distance**2-1):]], axis=1).astype(bool)\n",
    "\n",
    "\n",
    "\n",
    "detection_events, observable_flips = LogicalCircuit.compile_m2d_converter().convert(measurements=measurement_events, separate_observables=True)\n",
    "detection_events_signs = -1*np.sign(2*np.nanmean(detection_events.astype(int), axis=0)-1).astype(int)\n",
    "print(detection_events_signs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False,  True, False],\n",
       "       [False,  True, False, ..., False,  True, False],\n",
       "       [False, False, False, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True, False],\n",
       "       [ True,  True, False, ..., False, False,  True],\n",
       "       [False, False, False, ..., False,  True,  True]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "measurement_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_events_all_shots\n",
    "\n",
    "detection_events_signs = np.sign(np.nanmean(detection_events_all_shots,axis = -1))\n",
    "\n",
    "detection_events_signs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = 5\n",
    "# qubit_states_nans_perlog = np.load('2024_10_15_measurement_events_1CNOT_XX.npy').astype(bool)\n",
    "qubit_states_nans_perlog = np.load(f'/Users/gefenbaranes/Documents/2024_10_15_measurement_events_1CNOT_XX.npy')\n",
    "qubit_states_nans_perlog = qubit_states_nans_perlog.transpose(1,2,0)\n",
    "# exp_measurements = np.concatenate([exp_measurements[:, 0, :distance**2-1],\n",
    "#                             exp_measurements[:, 1, :distance**2-1],\n",
    "#                             exp_measurements[:, 0, distance**2-1:2*(distance**2-1)],\n",
    "#                             exp_measurements[:, 1, distance**2-1:2*(distance**2-1)],\n",
    "#                             exp_measurements[:, 0, 2*(distance**2-1):],\n",
    "#                             exp_measurements[:, 1, 2*(distance**2-1):]], axis=1)\n",
    "\n",
    "\n",
    "perfect_reps = np.ones(qubit_states_nans_perlog.shape[-1]).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_losses_per_stab_perlog' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kh/9b4nnp7x0s7c1fp6wn1qhrnm0000gn/T/ipykernel_62737/3085604329.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0mstabilizer_prod_per_round_per_stab_dataloss_postselection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlog\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_logicals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mstabilizer_prod_per_round_per_stab_dataloss_postselection\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstabilizer_prod_per_round\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_losses_per_stab_perlog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mperfect_reps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_losses_per_stab_perlog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_ancillas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0merror_prob_Astabs_fully_postselected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstabilizer_prod_per_round_per_stab_dataloss_postselection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mancilla_Astabs_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0merror_prob_Bstabs_fully_postselected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstabilizer_prod_per_round_per_stab_dataloss_postselection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mancilla_Bstabs_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/kh/9b4nnp7x0s7c1fp6wn1qhrnm0000gn/T/ipykernel_62737/3085604329.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0mstabilizer_prod_per_round_per_stab_dataloss_postselection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlog\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_logicals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mstabilizer_prod_per_round_per_stab_dataloss_postselection\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstabilizer_prod_per_round\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_losses_per_stab_perlog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mperfect_reps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_losses_per_stab_perlog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_ancillas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0merror_prob_Astabs_fully_postselected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstabilizer_prod_per_round_per_stab_dataloss_postselection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mancilla_Astabs_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0merror_prob_Bstabs_fully_postselected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstabilizer_prod_per_round_per_stab_dataloss_postselection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mancilla_Bstabs_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_losses_per_stab_perlog' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "num_data_blocks = num_logicals = 2\n",
    "\n",
    "num_ancilla_blocks = 2\n",
    "num_rounds = num_ancilla_blocks + 1\n",
    "\n",
    "ancilla_grid_size = 6\n",
    "data_grid_size = d = 5\n",
    "xspc = 3\n",
    "yspc = 2\n",
    "\n",
    "num_datas = 25\n",
    "num_ancillas = 24\n",
    "total_num_ancillas = num_ancillas*num_ancilla_blocks*num_logicals\n",
    "total_num_datas = num_datas*num_data_blocks\n",
    "num_physicals = total_num_ancillas + total_num_datas\n",
    "\n",
    "\n",
    "\n",
    "stabilizer_weights = np.zeros(num_ancillas)\n",
    "stabilizer_masks = np.zeros((num_ancillas, num_datas), dtype = bool)\n",
    "\n",
    "ancilla_Astabs_mask = Zstabs_mask =  np.array([1,1,0,1,0,1,0,0,1,0,1,0,0,1,0,1,0,0,1,0,1,0,1,1], dtype = bool) \n",
    "ancilla_Bstabs_mask = Xstabs_mask = (1-ancilla_Astabs_mask).astype(bool)\n",
    "\n",
    "deterministic_rounds = np.ones(num_rounds).astype(bool)\n",
    "nondeterministic_rounds = np.array([0] + (num_rounds - 2)*[1] + [0]).astype(bool)\n",
    "\n",
    "\n",
    "### HARDCODE FOR NOW\n",
    "stabilizer_weights = np.array([2., 2., 4., 4., 4., 4., 2., 2., 4., 4., 4., 4., 4., 4., 4., 4., 2.,\n",
    "       2., 4., 4., 4., 4., 2., 2.])\n",
    "stabilizer_masks = np.array([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "       [1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "       [0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0],\n",
    "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0],\n",
    "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0],\n",
    "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1],\n",
    "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
    "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]).astype(bool)\n",
    "\n",
    "\n",
    "stab2A = (ancilla_Astabs_mask & (stabilizer_weights == 2))\n",
    "stab2B = (ancilla_Bstabs_mask & (stabilizer_weights == 2))\n",
    "stab4A = (ancilla_Astabs_mask & (stabilizer_weights == 4))\n",
    "stab4B = (ancilla_Bstabs_mask & (stabilizer_weights == 4))\n",
    "\n",
    "\n",
    "vertical_string_masks = np.zeros((d,num_datas)).astype(bool)\n",
    "horizontal_string_masks = np.zeros((d,num_datas)).astype(bool)\n",
    "\n",
    "for i in range(d):\n",
    "    vertical_string_masks[i,i::d] = True\n",
    "    horizontal_string_masks[i,i*d:d*(i+1)] = True\n",
    "\n",
    "# qubit_states_nans = atoms_present_final.copy()\n",
    "\n",
    "\n",
    "# ## Reshape into per logical\n",
    "# qubit_states_nans_perlog = np.array([qubit_states_nans[logical_qubit_masks[i]] for i in range(num_logicals)])\n",
    "# qubit_states_perlog = np.nan_to_num(qubit_states_nans_perlog, nan = 3) #convert nan to 3 (so it then becomes 2 in the next line)\n",
    "# qubit_states_perlog = ((qubit_states_perlog + 1)/2).astype(int)\n",
    "\n",
    "# data_losses_perlog = np.isnan(qubit_states_nans_perlog[:,-num_datas:])\n",
    "# data_losses_per_stab_perlog = np.array([np.sum(data_losses_perlog[:,stabilizer_masks[i]], axis = 1) for i in range(num_ancillas)]).transpose(1,0,2)\n",
    "# num_data_losses_perlog = np.sum(data_losses_perlog, axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stabilizer_prod_per_round = []\n",
    "stabilizer_prod_per_round_withloss = []\n",
    "\n",
    "loss_sign = +1\n",
    "\n",
    "for r in range(-1,num_ancilla_blocks):\n",
    "    if r == -1:\n",
    "        stab_second = qubit_states_nans_perlog[:,num_ancillas*(r+1):num_ancillas*(r+2)]\n",
    "        stab_first = np.ones_like(stab_second)\n",
    "        stab_first_withloss = np.nan_to_num(stab_first,nan = loss_sign)\n",
    "        stab_second_withloss = np.nan_to_num(stab_second,nan = loss_sign)\n",
    "                                            \n",
    "    elif r == num_ancilla_blocks - 1:\n",
    "        stab_first = qubit_states_nans_perlog[:,num_ancillas*r:num_ancillas*(r+1)]\n",
    "        stab_first_withloss = np.nan_to_num(stab_first,nan = loss_sign)\n",
    "        stab_second = np.array([np.prod(qubit_states_nans_perlog[:,-num_datas:][:,stabilizer_masks[i]],axis = 1) for i in range(num_ancillas)]).transpose(1,0,2)\n",
    "        stab_second_withloss = np.array([np.prod(np.nan_to_num(qubit_states_nans_perlog[:,-num_datas:][:,stabilizer_masks[i]], nan = loss_sign),axis = 1) for i in range(num_ancillas)]).transpose(1,0,2)\n",
    "    else:\n",
    "        stab_first = qubit_states_nans_perlog[:,num_ancillas*r:num_ancillas*(r+1)]\n",
    "        stab_first_withloss = np.nan_to_num(stab_first,nan = loss_sign)\n",
    "        stab_second = qubit_states_nans_perlog[:,num_ancillas*(r+1):num_ancillas*(r+2)]\n",
    "        stab_second_withloss = np.nan_to_num(stab_second, nan = loss_sign)\n",
    "\n",
    "    stab_product_per_round = stab_first*stab_second\n",
    "    stab_product_per_round_withloss = stab_first_withloss*stab_second_withloss #convert loss to qubit state |0>\n",
    "\n",
    "    stabilizer_prod_per_round.append(stab_product_per_round)\n",
    "    stabilizer_prod_per_round_withloss.append(stab_product_per_round_withloss)\n",
    "\n",
    "stabilizer_prod_per_round = np.array(stabilizer_prod_per_round)\n",
    "stabilizer_prod_per_round_withloss = np.array(stabilizer_prod_per_round_withloss)\n",
    "\n",
    "# For sublattice A, and sublattice B separately\n",
    "\n",
    "error_prob_Astabs = 1-(1+np.abs(np.nanmean(stabilizer_prod_per_round[:,:,ancilla_Astabs_mask],axis = -1)))/2\n",
    "error_prob_Bstabs = 1-(1+np.abs(np.nanmean(stabilizer_prod_per_round[:,:,ancilla_Bstabs_mask], axis= -1)))/2\n",
    "\n",
    "error_prob_Astabs_postselected = 1-(1+np.abs(np.nanmean(stabilizer_prod_per_round[:,:,ancilla_Astabs_mask][:,:,:,perfect_reps],axis = -1)))/2\n",
    "error_prob_Bstabs_postselected = 1-(1+np.abs(np.nanmean(stabilizer_prod_per_round[:,:,ancilla_Bstabs_mask][:,:,:,perfect_reps], axis= -1)))/2\n",
    "\n",
    "error_prob_Astabs_fully_postselected = np.zeros_like(error_prob_Astabs_postselected)\n",
    "error_prob_Bstabs_fully_postselected = np.zeros_like(error_prob_Bstabs_postselected)\n",
    "\n",
    "stabilizer_prod_per_round_per_stab_dataloss_postselection = []\n",
    "for log in range(num_logicals):\n",
    "    stabilizer_prod_per_round_per_stab_dataloss_postselection += [[stabilizer_prod_per_round[:,log,i][:,data_losses_per_stab_perlog[log,i] == 0][:,perfect_reps[data_losses_per_stab_perlog[log,i] == 0]] for i in range(num_ancillas)]]\n",
    "    error_prob_Astabs_fully_postselected[:,log] = 1-(1+np.abs([np.nanmean(stabilizer_prod_per_round_per_stab_dataloss_postselection[-1][i],axis = -1) for i in np.where(ancilla_Astabs_mask)[0]]).T)/2\n",
    "    error_prob_Bstabs_fully_postselected[:,log] = 1-(1+np.abs([np.nanmean(stabilizer_prod_per_round_per_stab_dataloss_postselection[-1][i],axis = -1) for i in np.where(ancilla_Bstabs_mask)[0]]).T)/2\n",
    "\n",
    "plt.figure(\"Detected error per round\", figsize = (15,5))\n",
    "plt.clf()\n",
    "for log in range(num_logicals):  \n",
    "    Astab_good_rounds_thislog = deterministic_rounds if prep_basis[log] == 'vertical' else nondeterministic_rounds\n",
    "    Bstab_good_rounds_thislog = nondeterministic_rounds if prep_basis[log] == 'vertical' else deterministic_rounds\n",
    "    \n",
    "    plt.subplot(1,2,log+1)\n",
    "    plt.bar(np.arange(num_rounds)-0.2,np.nanmean(error_prob_Astabs[:,log],axis = -1),width = 0.4,label = \"A\", color = \"b\", alpha = 0.6)\n",
    "    plt.bar(np.arange(num_rounds)+0.2,np.nanmean(error_prob_Bstabs[:,log],axis = -1),width = 0.4,label = \"B\", color = \"orange\", alpha = 0.6)\n",
    "    plt.bar(np.arange(num_rounds)-0.2,np.nanmean(error_prob_Astabs_postselected[:,log],axis = -1),width = 0.4,label = \"A (post)\",color = \"b\")\n",
    "    plt.bar(np.arange(num_rounds)+0.2,np.nanmean(error_prob_Bstabs_postselected[:,log],axis = -1),width = 0.4,label = \"B (post)\", color = \"orange\")\n",
    "    \n",
    "    plt.xticks(np.arange(num_rounds))\n",
    "    plt.xlabel(\"Round\")\n",
    "    plt.ylabel(\"Detected error probability\")\n",
    "    plt.ylim(0,0.6)\n",
    "    plt.legend()\n",
    "    plt.title(f\"\"\"Average error prob (no postselection, exclude loss): {np.nanmean(np.concatenate((error_prob_Astabs[:,log][Astab_good_rounds_thislog],error_prob_Bstabs[:,log][Bstab_good_rounds_thislog]))):.4f}\n",
    "    Average error prob (postselected, exclude loss): {np.nanmean(np.concatenate((error_prob_Astabs_postselected[:,log][Astab_good_rounds_thislog],error_prob_Bstabs_postselected[:,log][Bstab_good_rounds_thislog]))):.4f}\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        1, 0, 0, 1, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0, 1, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 0, 1, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_prob_Astabs_postselected_weight_2 = 1-(1+np.abs(np.nanmean(stabilizer_prod_per_round[:,:,stab2A][:,:,:,perfect_reps],axis = -1)))/2\n",
    "error_prob_Bstabs_postselected_weight_2  = 1-(1+np.abs(np.nanmean(stabilizer_prod_per_round[:,:,stab2B][:,:,:,perfect_reps], axis= -1)))/2\n",
    "error_prob_Astabs_postselected_weight_4 = 1-(1+np.abs(np.nanmean(stabilizer_prod_per_round[:,:,stab4A][:,:,:,perfect_reps],axis = -1)))/2\n",
    "error_prob_Bstabs_postselected_weight_4  = 1-(1+np.abs(np.nanmean(stabilizer_prod_per_round[:,:,stab4B][:,:,:,perfect_reps], axis= -1)))/2\n",
    "\n",
    "error_prob_Astabs_postselected_weight_2_err = np.nanstd(stabilizer_prod_per_round[:,:,stab2A][:,:,:,perfect_reps],axis = -1)/2/np.sqrt(stabilizer_prod_per_round[:,:,stab2A][:,:,:,perfect_reps].shape[-1])\n",
    "error_prob_Bstabs_postselected_weight_2_err  = np.nanstd(stabilizer_prod_per_round[:,:,stab2B][:,:,:,perfect_reps], axis= -1)/2/np.sqrt(stabilizer_prod_per_round[:,:,stab2B][:,:,:,perfect_reps].shape[-1])\n",
    "error_prob_Astabs_postselected_weight_4_err = np.nanstd(stabilizer_prod_per_round[:,:,stab4A][:,:,:,perfect_reps],axis = -1)/2/np.sqrt(stabilizer_prod_per_round[:,:,stab4A][:,:,:,perfect_reps].shape[-1])\n",
    "error_prob_Bstabs_postselected_weight_4_err  = np.nanstd(stabilizer_prod_per_round[:,:,stab4B][:,:,:,perfect_reps], axis= -1)/2/np.sqrt(stabilizer_prod_per_round[:,:,stab4B][:,:,:,perfect_reps].shape[-1])\n",
    "\n",
    "stab_mean_prod_stab2A = np.array([np.abs([np.nanmean(stabilizer_prod_per_round_per_stab_dataloss_postselection[log][i],axis = -1) for i in np.where(stab2A)[0]]).T for log in range(num_logicals)]).transpose(1,0,2)\n",
    "stab_mean_prod_stab2B = np.array([np.abs([np.nanmean(stabilizer_prod_per_round_per_stab_dataloss_postselection[log][i],axis = -1) for i in np.where(stab2B)[0]]).T for log in range(num_logicals)]).transpose(1,0,2)\n",
    "stab_mean_prod_stab4A = np.array([np.abs([np.nanmean(stabilizer_prod_per_round_per_stab_dataloss_postselection[log][i],axis = -1) for i in np.where(stab4A)[0]]).T for log in range(num_logicals)]).transpose(1,0,2)\n",
    "stab_mean_prod_stab4B = np.array([np.abs([np.nanmean(stabilizer_prod_per_round_per_stab_dataloss_postselection[log][i],axis = -1) for i in np.where(stab4B)[0]]).T for log in range(num_logicals)]).transpose(1,0,2)\n",
    "\n",
    "error_prob_Astabs_fully_postselected_weight_2 = 1-(1+np.abs(stab_mean_prod_stab2A))/2\n",
    "error_prob_Bstabs_fully_postselected_weight_2  = 1-(1+np.abs(stab_mean_prod_stab2B))/2\n",
    "error_prob_Astabs_fully_postselected_weight_4 = 1-(1+np.abs(stab_mean_prod_stab4A))/2\n",
    "error_prob_Bstabs_fully_postselected_weight_4  = 1-(1+np.abs(stab_mean_prod_stab4B))/2\n",
    "\n",
    "error_prob_Astabs_fully_postselected_weight_2_err = np.array([0.5 * np.array([np.nanstd(stabilizer_prod_per_round_per_stab_dataloss_postselection[log][i],axis = -1)/np.sqrt(stabilizer_prod_per_round_per_stab_dataloss_postselection[log][i].shape[-1]) \n",
    "                                                                              for i in np.where(stab2A)[0]]).T for log in range(num_logicals)]).transpose(1,0,2)\n",
    "error_prob_Bstabs_fully_postselected_weight_2_err = np.array([0.5 * np.array([np.nanstd(stabilizer_prod_per_round_per_stab_dataloss_postselection[log][i],axis = -1)/np.sqrt(stabilizer_prod_per_round_per_stab_dataloss_postselection[log][i].shape[-1]) \n",
    "                                                                              for i in np.where(stab2B)[0]]).T for log in range(num_logicals)]).transpose(1,0,2)\n",
    "error_prob_Astabs_fully_postselected_weight_4_err = np.array([0.5 * np.array([np.nanstd(stabilizer_prod_per_round_per_stab_dataloss_postselection[log][i],axis = -1)/np.sqrt(stabilizer_prod_per_round_per_stab_dataloss_postselection[log][i].shape[-1]) \n",
    "                                                                              for i in np.where(stab4A)[0]]).T for log in range(num_logicals)]).transpose(1,0,2)\n",
    "error_prob_Bstabs_fully_postselected_weight_4_err  = np.array([0.5 * np.array([np.nanstd(stabilizer_prod_per_round_per_stab_dataloss_postselection[log][i],axis = -1)/np.sqrt(stabilizer_prod_per_round_per_stab_dataloss_postselection[log][i].shape[-1]) \n",
    "                                                                              for i in np.where(stab4B)[0]]).T for log in range(num_logicals)]).transpose(1,0,2)\n",
    "\n",
    "\n",
    "error_prob_Astabs_postselected_withloss_weight_2  = 1-(1+np.abs(np.mean(stabilizer_prod_per_round_withloss[:,:,stab2A][:,:,:,perfect_reps],axis = -1)))/2\n",
    "error_prob_Bstabs_postselected_withloss_weight_2  = 1-(1+np.abs(np.mean(stabilizer_prod_per_round_withloss[:,:,stab2B][:,:,:,perfect_reps], axis= -1)))/2\n",
    "error_prob_Astabs_postselected_withloss_weight_4  = 1-(1+np.abs(np.mean(stabilizer_prod_per_round_withloss[:,:,stab4A][:,:,:,perfect_reps],axis = -1)))/2\n",
    "error_prob_Bstabs_postselected_withloss_weight_4  = 1-(1+np.abs(np.mean(stabilizer_prod_per_round_withloss[:,:,stab4B][:,:,:,perfect_reps], axis= -1)))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,10))\n",
    "fig.subplots_adjust(hspace = 0.4)\n",
    "\n",
    "for log in range(num_logicals):  \n",
    "    Astab_good_rounds_thislog = deterministic_rounds if prep_basis[log] == 'vertical' else nondeterministic_rounds\n",
    "    Bstab_good_rounds_thislog = nondeterministic_rounds if prep_basis[log] == 'vertical' else deterministic_rounds\n",
    "    \n",
    "    plt.subplot(num_logicals,2,log*num_logicals + 1)\n",
    "\n",
    "    plt.plot(np.arange(num_rounds)[Astab_good_rounds_thislog], error_prob_Astabs_postselected_weight_2[Astab_good_rounds_thislog, log],'o--',color='steelblue', alpha = 0.2, zorder = -20)\n",
    "    plt.plot(np.arange(num_rounds)[Bstab_good_rounds_thislog], error_prob_Bstabs_postselected_weight_2[Bstab_good_rounds_thislog, log],'o--',color='steelblue', alpha = 0.2, zorder = -20)\n",
    "    plt.plot(np.arange(num_rounds)[Astab_good_rounds_thislog], error_prob_Astabs_postselected_weight_4[Astab_good_rounds_thislog, log],'o--',color='firebrick', alpha = 0.2, zorder = -20)\n",
    "    plt.plot(np.arange(num_rounds)[Bstab_good_rounds_thislog], error_prob_Bstabs_postselected_weight_4[Bstab_good_rounds_thislog, log],'o--',color='firebrick', alpha = 0.2, zorder = -20)\n",
    "    plt.plot(np.arange(num_rounds)[Astab_good_rounds_thislog], np.mean(error_prob_Astabs_postselected_weight_2,axis=-1)[Astab_good_rounds_thislog, log],'o-',color='steelblue')\n",
    "    plt.plot(np.arange(num_rounds)[Bstab_good_rounds_thislog], np.mean(error_prob_Bstabs_postselected_weight_2,axis=-1)[Bstab_good_rounds_thislog, log],'o-',color='steelblue', label='weight 2')\n",
    "    plt.plot(np.arange(num_rounds)[Astab_good_rounds_thislog], np.mean(error_prob_Astabs_postselected_weight_4,axis=-1)[Astab_good_rounds_thislog, log],'o-',color='firebrick')\n",
    "    plt.plot(np.arange(num_rounds)[Bstab_good_rounds_thislog], np.mean(error_prob_Bstabs_postselected_weight_4,axis=-1)[Bstab_good_rounds_thislog, log],'o-',color='firebrick', label='weight 4')\n",
    "    plt.legend(frameon=False)\n",
    "    plt.ylim(0, 0.5)\n",
    "    plt.xticks([0,1,2])\n",
    "    plt.xlabel('QEC cycle')\n",
    "    plt.ylabel(\"Detection probability\")\n",
    "    \n",
    "    all_good_weight2 = np.concatenate((error_prob_Astabs_postselected_weight_2[Astab_good_rounds_thislog, log], error_prob_Bstabs_postselected_weight_2[Bstab_good_rounds_thislog, log])).ravel()\n",
    "    all_good_weight4 =  np.concatenate((error_prob_Astabs_postselected_weight_4[Astab_good_rounds_thislog, log], error_prob_Bstabs_postselected_weight_4[Bstab_good_rounds_thislog, log])).ravel()\n",
    "    \n",
    "    all_good_weight2_err = np.concatenate((error_prob_Astabs_postselected_weight_2_err[Astab_good_rounds_thislog, log], error_prob_Bstabs_postselected_weight_2_err[Bstab_good_rounds_thislog, log])).ravel()\n",
    "    all_good_weight4_err = np.concatenate((error_prob_Astabs_postselected_weight_4_err[Astab_good_rounds_thislog, log], error_prob_Bstabs_postselected_weight_4_err[Bstab_good_rounds_thislog, log])).ravel()\n",
    "    \n",
    "    plt.title(f\"\"\"Logical {log+1}: Postselected on rearrangement only\n",
    "    Weight 2: {np.mean(all_good_weight2):.4f}({np.mean(all_good_weight2_err)/np.sqrt(all_good_weight2.size):.4f}), Weight 4: {np.mean(all_good_weight4):.4f}({np.mean(all_good_weight4_err)/np.sqrt(all_good_weight4.size):.4f})\n",
    "    Overall {np.mean(np.concatenate((all_good_weight2, all_good_weight4))):.4f}({np.mean(np.concatenate((all_good_weight2_err, all_good_weight4_err)))/np.sqrt(np.concatenate((all_good_weight2, all_good_weight4)).ravel().size):.4f})\"\"\")\n",
    "    \n",
    "    plt.subplot(num_logicals,2,log*num_logicals + 2)\n",
    "\n",
    "    plt.plot(np.arange(num_rounds)[Astab_good_rounds_thislog], error_prob_Astabs_fully_postselected_weight_2[Astab_good_rounds_thislog, log],'o--',color='steelblue', alpha = 0.2, zorder = -20)\n",
    "    plt.plot(np.arange(num_rounds)[Bstab_good_rounds_thislog], error_prob_Bstabs_fully_postselected_weight_2[Bstab_good_rounds_thislog, log],'o--',color='steelblue', alpha = 0.2, zorder = -20)\n",
    "    plt.plot(np.arange(num_rounds)[Astab_good_rounds_thislog], error_prob_Astabs_fully_postselected_weight_4[Astab_good_rounds_thislog, log],'o--',color='firebrick', alpha = 0.2, zorder = -20)\n",
    "    plt.plot(np.arange(num_rounds)[Bstab_good_rounds_thislog], error_prob_Bstabs_fully_postselected_weight_4[Bstab_good_rounds_thislog, log],'o--',color='firebrick', alpha = 0.2, zorder = -20)\n",
    "    plt.plot(np.arange(num_rounds)[Astab_good_rounds_thislog], np.mean(error_prob_Astabs_fully_postselected_weight_2,axis=-1)[Astab_good_rounds_thislog, log],'o-',color='steelblue')\n",
    "    plt.plot(np.arange(num_rounds)[Bstab_good_rounds_thislog], np.mean(error_prob_Bstabs_fully_postselected_weight_2,axis=-1)[Bstab_good_rounds_thislog, log],'o-',color='steelblue', label='weight 2')\n",
    "    plt.plot(np.arange(num_rounds)[Astab_good_rounds_thislog], np.mean(error_prob_Astabs_fully_postselected_weight_4,axis=-1)[Astab_good_rounds_thislog, log],'o-',color='firebrick')\n",
    "    plt.plot(np.arange(num_rounds)[Bstab_good_rounds_thislog], np.mean(error_prob_Bstabs_fully_postselected_weight_4,axis=-1)[Bstab_good_rounds_thislog, log],'o-',color='firebrick', label='weight 4')\n",
    "    plt.legend(frameon=False)\n",
    "    plt.ylim(0, 0.5)\n",
    "    plt.xticks([0,1,2])\n",
    "    plt.xlabel('QEC cycle')\n",
    "    plt.ylabel(\"Detection probability\")\n",
    "    all_good_weight2 = np.concatenate((error_prob_Astabs_fully_postselected_weight_2[Astab_good_rounds_thislog, log], error_prob_Bstabs_fully_postselected_weight_2[Bstab_good_rounds_thislog, log])).ravel()\n",
    "    all_good_weight4 =  np.concatenate((error_prob_Astabs_fully_postselected_weight_4[Astab_good_rounds_thislog, log], error_prob_Bstabs_fully_postselected_weight_4[Bstab_good_rounds_thislog, log])).ravel()\n",
    "    \n",
    "    all_good_weight2_err = np.concatenate((error_prob_Astabs_fully_postselected_weight_2_err[Astab_good_rounds_thislog, log], error_prob_Bstabs_fully_postselected_weight_2_err[Bstab_good_rounds_thislog, log])).ravel()\n",
    "    all_good_weight4_err = np.concatenate((error_prob_Astabs_fully_postselected_weight_4_err[Astab_good_rounds_thislog, log], error_prob_Bstabs_fully_postselected_weight_4_err[Bstab_good_rounds_thislog, log])).ravel()\n",
    "    \n",
    "    plt.title(f\"\"\"Logical {log + 1}: Postselected on rearrangement and no data loss\n",
    "    Weight 2: {np.mean(all_good_weight2):.4f}({np.mean(all_good_weight2_err)/np.sqrt(all_good_weight2.size):.4f}), Weight 4: {np.mean(all_good_weight4):.4f}({np.mean(all_good_weight4_err)/np.sqrt(all_good_weight4.size):.4f})\n",
    "    Overall {np.mean(np.concatenate((all_good_weight2, all_good_weight4))):.4f}({np.mean(np.concatenate((all_good_weight2_err, all_good_weight4_err)))/np.sqrt(np.concatenate((all_good_weight2, all_good_weight4)).ravel().size):.4f})\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for log in np.arange(num_logicals):\n",
    "    fig, axs = plt.subplots(ancilla_grid_size,ancilla_grid_size)\n",
    "    # fig.canvas.set_window_title(\"Ancilla stabilizers, excluding loss\")\n",
    "    fig.subplots_adjust(wspace=0.3,hspace=0.3)\n",
    "    count = 0\n",
    "    for i in range(ancilla_grid_size**2):\n",
    "        ax = axs[i//ancilla_grid_size,i%ancilla_grid_size]\n",
    "        if ancilla_block_mask[i]:\n",
    "            mean_stab = np.nanmean(stabilizer_prod_per_round[:,log,count,perfect_reps], axis = -1)\n",
    "            # ax.bar(np.arange(num_rounds),mean_stab, color = [\"r\",\"k\"][int(ancilla_Astabs_mask[count])])\n",
    "            ax.bar(np.arange(num_rounds),mean_stab, color = [\"r\",\"k\"][int(ancilla_Astabs_mask[count])])\n",
    "            ax.set_xticks([])\n",
    "            ax.set_ylim(-1,1)\n",
    "            count+=1 \n",
    "        else:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    fig.suptitle(\"Logical %i stab. products (perfect rearrangement, exclude loss)\"%(log+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "A = 'logical_CX_NL1_NCX2'\n",
    "\n",
    "print(int(A[13:14]))\n",
    "\n",
    "print(int(A[18:19]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
